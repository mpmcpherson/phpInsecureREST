Development and testing best practices

1. YAGNI: "You Aint Gonna Need It". Don't write code that you think you might need in future, but don't need yet. This is coding for imaginary future use cases, and inevitably the code will become dead code or need rewriting because the future use case always turns out to work slightly differently from how you imagined it.

If you put code in for a future use case, I will question it in a code review. (You can, and must, design APIs, for example, to permit future use cases, but that's a different issue.)

The same is true for commenting-out code; if a block of commented code is going into a release, it shouldn't exist. If it is code that may be restored, make a ticket and reference the commit hash for the code delete. YAGNI is a core element of agile programming. The best reference for this is Extreme Programming Explained, by Kent Beck.

2. Tests don't need testing. Infrastructure, frameworks, and libraries for testing need tests. Don't test the browser or external libraries unless you really need to. Test the code you write, not other people’s code.

3. The third time you write the same piece of code is the right time to extract it into a general-purpose helper (and write tests for it). Helper functions within a test don't need testing; when you break them out and reuse them they do need tests. By the third time you've written similar code, you tend to have a clear idea of what shape the general-purpose problem is that you're solving.

4. When it comes to API design (external facing and object API): Simple things should be simple; complex things should be possible. Design for the simple case first, with preferably zero configuration or parameterization, if that's possible. Add options or additional API methods for more complex and flexible use cases (as they are needed).

5. Fail fast. Check input and fail on nonsensical input or invalid state as early as possible, preferably with an exception or error response that will make the exact problem clear to your caller. Permit "innovative" use cases of your code though (i.e., don't do type checking for input validation unless you really need to).

6. Unit tests test to the unit of behavior, not the unit of implementation. Changing the implementation, without changing the behavior or having to change any of your tests is the goal, although not always possible. So where possible, treat your test objects as black boxes, testing through the public API without calling private methods or tinkering with state.

For some complex scenarios—such as testing behavior on a specific complex state to find an obscure bug—that may not be possible. Writing tests first really helps with this as it forces you to think about the behavior of your code and how you're going to test it before you write it. Testing first encourages smaller, more modular units of code, which generally means better code. A good reference for getting started with the "test first" approach is Test Driven Development by Example, by Kent Beck.

7. For unit tests (including test infrastructure tests) all code paths should be tested. 100% coverage is a good place to start. You can't cover all possible permutations/combinations of state (combinatorial explosion), so that requires consideration. Only if there is a very good reason should code paths be left untested. Lack of time is not a good reason and ends up costing more time. Possible good reasons include: genuinely untestable (in any meaningful way), impossible to hit in practice, or covered elsewhere in a test. Code without tests is a liability. Measuring coverage and rejecting PRs that reduce coverage percentage is one way to ensure you make gradual progress in the right direction.

8. Code is the enemy: It can go wrong, and it needs maintenance. Write less code. Delete code. Don’t write code you don’t need.

9. Inevitably, code comments become lies over time. In practice, few people update comments when things change. Strive to make your code readable and self-documenting through good naming practices and known programming style.

Code that can't be made obvious—working around an obscure bug or unlikely condition, or a necessary optimization—does need commenting. Comment the intent of the code, and why it is doing something rather than what it is doing. (This particular point about comments being lies is controversial, by the way. I still think it’s correct, and Kernighan and Pike, authors of The Practice of Programming, agree with me.)

10. Write defensively. Always think about what can go wrong, what will happen on invalid input, and what might fail, which will help you catch many bugs before they happen.

11. Logic is easy to unit test if it is stateless and side-effect free. Break out logic into separate functions, rather than mixing logic into stateful and side-effect-filled code. Separating stateful code and code with side-effects into smaller functions makes them easier to mock out and unit test without side-effects. (Less overhead for tests means faster tests.) Side effects do need testing, but testing them once and mocking them out everywhere else is generally a good pattern.

12. Globals are bad. Functions are better than types. Objects are likely to be better than complex data structures.

13. Using the Python built-in types—and their methods—will be faster than writing your own types (unless you're writing in C). If performance is a consideration, try to work out how to use the standard built-in types rather than custom objects.

14. Dependency injection is a useful coding pattern for being clear about what your dependencies are and where they come from. (Have objects, methods, and so on receive their dependencies as parameters rather than instantiating new objects themselves.) This does make API signatures more complex, so it is a trade-off. Ending up with a method that needs 10 parameters for all its dependencies is good sign your code is doing too much, anyway. The definitive article on dependency injection is "Inversion of Control Containers and the Dependency Injection Pattern," by Martin Fowler.

15. The more you have to mock out to test your code, the worse your code is. The more code you have to instantiate and put in place to be able to test a specific piece of behavior, the worse your code is. The goal is small testable units, along with higher-level integration and functional tests to test that the units cooperate correctly.

16. External-facing APIs are where "design up front"—and consideration about future use cases—really matters. Changing APIs is a pain for us and for our users, and creating backwards incompatibility is horrible (although sometimes impossible to avoid). Design external facing APIs carefully, still keeping to the "simple things should be simple" principle.

17. If a function or method goes past 30 lines of code, consider breaking it up. A good maximum module size is about 500 lines. Test files tend to be longer than this.

18. Don’t do work in object constructors, which are hard to test and surprising. Don’t put code in __init__.py (except imports for namespacing). __init__.py is not where programmers generally expect to find code, so it’s "surprising."

19. DRY (Don’t Repeat Yourself) matters much less in tests than it does in production code. Readability of an individual test file is more important than maintainability (breaking out reusable chunks). That’s because tests are executed and read individually rather than themselves being part of a larger system. Obviously excessive repetition means reusable components can be created for convenience, but it’s much less of a concern than it is for production.

20. Refactor whenever you see the need and have the chance. Programming is about abstractions, and the closer your abstractions map to the problem domain, the easier your code is to understand and maintain. As systems grow organically, they need to change structure for their expanding use case. Systems outgrow their abstractions and structure, and not changing them becomes technical debt that is more painful (and slower and more buggy) to work around. Include the cost of clearing technical debt (refactoring) within the estimates for feature work. The longer you leave the debt around, the higher the interest it accumulates. A great book on refactoring and testing is Working Effectively with Legacy Code, by Michael Feathers.

21. Make code correct first and fast second. When working on performance issues, always profile before making fixes. Usually the bottleneck is not quite where you thought it was. Writing obscure code because it is faster is only worth it if you’ve profiled and proven that it’s actually worth it. Writing a test that exercises the code you’re profiling with timing around it makes knowing when you’re done easier, and can be left in the test suite to prevent performance regressions. (With the usual note that adding timing code always changes the performance characteristics of the code, making performance work one of the more frustrating tasks.)

22. Smaller, more tightly scoped unit tests give more valuable information when they fail—they tell you specifically what is wrong. A test that stands up half the system to test behavior takes more investigation to determine what is wrong. Generally a test that takes more than 0.1 seconds to run isn’t a unit test. There’s no such thing as a slow unit test. With tightly scoped unit tests testing behavior, your tests act as a de facto specification for your code. Ideally if someone wants to understand your code, they should be able to turn to the test suite as "documentation" for the behavior. A great presentation on unit testing practices is Fast Test, Slow Test, by Gary Bernhardt:

​

23. "Not Invented Here" is not as bad as people say. If we write the code, then we know what it does, we know how to maintain it, and we’re free to extend and modify it as we see fit. This follows the YAGNI principle: We have specific code for the use cases we need rather than general purpose code that has complexity for things we don’t need. On the other hand, code is the enemy, and owning more code than necessary is bad. Consider the trade-off when introducing a new dependency.

24. Shared code ownership is the goal; siloed knowledge is bad. At a minimum, this means discussing or documenting design decisions and important implementation decisions. Code review is the worst time to start discussing design decisions as the inertia to make sweeping changes after code has been written is hard to overcome. (Of course it’s still better to point out and change design mistakes at review time than never.)

25. Generators rock! They’re generally shorter and easier to understand than stateful objects for iteration or repeated execution. A good introduction to generators is "Generator Tricks for Systems Programmers," by David Beazley.

26. Let’s be engineers! Let’s think about design and build robust and well-implemented systems, rather than growing organic monsters. Programming is a balancing act, however. We’re not always building a rocket ship. Over-engineering (onion architecture) is as painful to work with as under-designed code. Almost anything by Robert Martin is worth reading, and Clean Architecture: A Craftsman’s Guide to Software Structure and Design is a good resource on this topic. Design Patterns is a classic programming book that every engineer should read.

27. Intermittently failing tests erode the value of your test suite, to the point in which eventually everyone ignores test run results because there’s always something failing. Fixing or deleting intermittently failing tests is painful, but worth the effort.

28. Generally, particularly in tests, wait for a specific change rather than sleeping for an arbitrary amount of time. Voodoo sleeps are hard to understand and slow down your test suite.

29. Always see your test fail at least once. Put a deliberate bug in and make sure it fails, or run the test before the behavior under test is complete. Otherwise you don’t know that you’re really testing anything. Accidentally writing tests that actually don’t test anything or that can never fail is easy.

30. And finally, a point for management: Constant feature grind is a terrible way to develop software. Not letting developers take pride in their work ensures you won’t get the best out of them. Not addressing technical debt slows down development and results in a worse, more buggy product.



 Presentation, Blog Post
By Ehis Ojielu

Is there an "ideal software development project," and, if so, what are steps you should take to achieve this ideal state? There are strict guidelines and different software development best practices methodologies such as scrum or extreme programming, but I have come to the realization that it's not always possible – or wise – to strictly follow these processes.

It doesn’t imply that we don’t strive to accurately implement these methods; we just need to stay flexible. As part of this flexibility, we should think about overall structures that make a project successful and how consistency and coherence can improve your odds of achieving the "ideal." Because there isn't a "playbook" on the components of the ideal software development project, we pulled together some of our most tried and true best practices that make software development projects at Dialexa work better:
1. CODE SIMPLICITY
Strive to keep your code simple.

Code simplicity is an idea that came from Max Kanat-Alexander, a software developer at Google and Community Lead and Release Manager of the Bugzilla project. The idea is to reduce unnecessary complexity in software development. The code simplicity movement goes hand in hand with other software principles such as DRY (Don’t Repeat Yourself), introduced in the book The Pragmatic Programmer, and YAGNI (You Aren’t Gonna Need It), a mantra in agile development. Max has some interesting posts on this idea in his Code Simplicity blog.
Five Best Practices for Software Development Projects from Dialexa

FIVE BEST PRACTICES FOR SOFTWARE DEVELOPMENT PROJECTS FROM DIALEXA
2. TESTING
Continuously test from end to end.

Initially I was a skeptic of Test Driven Development (TDD) as it seemed too prescriptive. Over time, I have realized that TDD gives you more confidence regarding your code quality. On the other hand, Behavior Driven Development (BDD) allows you to learn the features and requirements directly from the customer and that alignment translates into code that is closer to the users’ needs. Full integration testing ensures that all components are working together as expected and increases code coverage.
3. CODE COHERENCE
Keep it consistent across your team.

When working with a team, it's important to have a consistent style guide for your codebase. If you have a codebase where you CAN tell who wrote a particular file then there isn’t consistency between authors. There are many tools to enforce consistent style; here are three we like:

    JSCS (Javascript Style Checker) is a JavaScript linter which also has a great formatter.

    ESLint is an extremely configurable linter and has gained a lot of popularity in the community.

    Editorconfig is a tool to enable consistency between the many editors and IDE’s that your developers use.

ComicStrip1.png
4. CODE REVIEWS
Don’t be shy, allow someone to check your code!

Everyone makes mistakes. An attitude which allows you to acknowledge imperfections is the first step to investing your trust in a code review. Having a colleague read over your pull requests before merging is a good way to ensure final code quality. Code reviews help reduce bugs in the product – that's the bottom line - so give up that idea of perfection. “The Code Review Mindset" is a great article on the importance of code reviews.

Find out where great products come from with our End-to-End Product Development Guide.
5. ESTIMATION
Set your time and budget estimates realistically.

A realistic budget keeps your software project from feeling too much pressure. With agile methods, this enables the scope to flex more easily as the project progresses, but an estimate that is truly off can cause problems in quality, morale and output. Estimation can be quite tricky – as it's hard to find a happy balance between being realistic and sandbagging when there are so many unknowns. Rest assured, better estimation comes with experience, and there are many tools available to assist with software development project coding estimates. Here is a good list to start.


Testing is essential

Every programmer knows what it’s like to go on a hack attack, spewing out lines of code like a machine gun. When you’ve got a grand vision of the architecture in your head, you can never turn it into code quickly enough. This is what it was like when Michelangelo painted the Sistine Chapel, but in one 72-hour stretch. 

While the code from a bender can often be brilliant, it’s usually hairy and unfinished in places. To make matters worse, the author doesn’t always remember the places where there is a gap left to be filled in later. For all of the code's grand artistry, it’s not ready to ship. The way we turn our rough first drafts into finished code is with disciplined, rigorous testing using one of the top-rated application security testing tools. 

Over the last decade, testing has become better than ever as development teams have created strong protocols and built automation features to enforce them. Teams are using new continuous integration mechanisms that take our code and start poking and prodding it as soon as we check it in. As long as we build good unit tests — which is its own type of challenge — the testing automation robots will make sure that our code moves forward. If we make mistakes, it will catch them and hassle us until they’re fixed. And when our code sails through all of the unit tests, we can be sure that it won’t fail — at least not in the ways that we’ve anticipated when writing the tests. There’s no guarantee that the code is truly bug-free, but testing rigor ensures that we’ve gotten the obvious ones.  
Repositories let us fix our mistakes

How many times have you made a mistake and wished you could go back in time to fix it? We’ve gone down a path, ripping apart the code and gluing in new structures, only to discover that it was all a mistake. The original code was better. 

Luckily, we were committing the work to a version control system during the coding process. Good version control repositories like CVS, Subversion, and Git make it possible to experiment with code and improve it without needing to worry that we might be heading in the wrong direction. The repository tracks the evolution of the code and lets us go back in time if it was all a mistake.  

The repositories also let us synchronize our work on projects, tracking the differences and making it possible to merge our work with others when the time comes. Without this steady, neutral service knitting the work together, teams would find it much harder to build reliable code, and they’d be to afraid to experiment with new features.  
Development methodologies matter

Bold hearts may leap into the abyss, but rational minds develop a strategic plan for descending gently into it. We would not be able to build large or medium software projects without a careful way to merge all of these crazy instincts, intuitions, and dreams into a rational, thoroughly planned workflow.  

There are ongoing arguments about the different types of programming methodologies, many of them in opposition to each other. There are, for instance, those who believe wholeheartedly that good software can’t be built without the flexibility of agile methods. Then there are others who toss agile aside because it’s too capricious and arbitrary. Personally, there are a lot of aspects about the agile process that I like, but I’ve seen it go astray when too many programmers wander off the ranch. 

Software engineers have thought long and hard about how we’re supposed to work together to write code. They haven’t reached a consensus, but that doesn’t mean that the ideas aren’t better than nothing. Our ambitions are so large these days that we need dozens, if not hundreds or thousands, of people working together, and without coordination it would be chaos. You can choose whichever side you like — as long as you choose one. 
Code must live on

Software has flaws and limitations, but age is not one of them. Steel rusts and organic material spoils, but the logic in software lives on. There are, as we speak, IBM mainframes running COBOL code written by people who never lived long enough to send a Tweet or post a status update on Facebook. They may be gone, but their code lives on.

Applications only have age-related issues when they are no longer compatible with current systems or they don’t have the new features and updates in the current software product. Only code maintenance allows old applications to remain useful.

Good code maintenance begins with good engineering. When teams write well-documented code with modular interfaces, their work can keep running and running. Software engineering makes it possible for a part of us to live on. It’s not the same as downloading our soul into the matrix, but it does bear a resemblance.
Code analysis 

Long ago, I made the mistake of putting too much faith in Rice’s theorem, a deep part of computer science that states that one computer program can’t analyze another computer program and decide whether it has some nontrivial property. And the theorem means “nontrivial” in the most abstract sense. If some property is true for some programs and not for others, it’s considered “nontrivial.”

The theorem would seem to suggest that it’s futile to spend any time trying to write code that would look for bugs or find errors. But the theorem actually says that machine intelligence can’t do that correctly all of the time. I assumed that no code could look for bugs and find errors.

Software engineers aren’t as confused by deep theoretical results. They understand that it’s possible to write software that will scan our code and look for common mistakes or poor practices. Good tools can look for sloppy errors like uninitialized variables and deeper problems like buffer overruns or SQL injection vulnerabilities.  

This is the lesson of good engineering. It doesn’t need to be perfect. It doesn’t need to be deep. It doesn’t need to be jaw-dropping. It just needs to be built carefully with a diligent and methodical focus on correctness. The process may never correct all of the bugs all of the time, but that doesn’t mean that we can’t be happy finding some of them. If this process is repeated, we can get close enough.



To succeed with distributed rapid development, a branch-merge strategy is key. A good strategy facilitates processes among multiple developers or teams and is the basis for any well-functioning DevOps pipeline that uses continuous integration (CI).

While there are many ways to implement CI and DevOps without a branch-merge strategy, most mature organizations use them to shift code defects left and reduce the negative impacts of a defect for those working on the particular change.

Let’s explore branching strategies, merging strategies, and how you can put them together in a way that’s right for your team in order to bring quality features to production faster.
Branching Strategies

Irrespective of your version control tool, a good branching strategy helps teams of developers to easily collaborate, while not allowing disruptive or code-breaking changes by one developer to impact other code. Changes to the branch don't affect other developers until the developer or team has tested the changes and decides to merge the code. Developers can still pull down changes from other developers to collaborate on features and ensure their private branch doesn’t diverge too far from the main code line. 

Branching models may differ between organizations, but there are four strategies that are most commonly implemented. Choosing the right strategy is paramount to a successful implementation.
Trunk-based Development (No Branching)

Trunk-based development means all developers work on the same branch, and when changes are tested and ready, a developer pushes their code to the central repository. Small organizations or those with strong internal testing practices find this strategy useful because it reduces complexity and encourages the development organization to swarm on the problem.

If your program is interested in utilizing feature toggles in your implementation, this strategy could be very effective. Unfortunately, it does come with risks. Large multi-team development can struggle with this strategy, as one defect can halt all forward progress until the trunk is fixed, causing unnecessary churn or delays—which are compounded if your compnay culture exhibits a “blame and deflect” tendency to defect resolution.
Release Branching

Release branching refers to the idea that a release is contained within a branch. When a team starts working on a new release, a branch is created (e.g., “1.1 development branch” or “Release 2.1”), and all work done until the next release is stored in this branch.

This strategy is most commonly used in waterfall and Scrummerfall development processes. Release branching can be unwieldy and hard to manage if you have many people working on the same branch. Unless you have very small release cycles of less than two weeks, release branches nearly always ensure late cycle development, release delays, long testing cycles, and challenges integrating multiple features.
Feature Branching

Feature branches, which are often coupled with feature flags or toggles that enable and disable a feature within the product, are used to collect a series of user stories that can be merged into a master and deployed as one complete feature. This makes it easier to move toward a continuous delivery process, and, if used in conjunction with toggles, it’s also easy to decide when to expose end-users to new features.

This type of approach reduces the time of delivery and testing cycles. A mature software development lifecycle is required to implement feature branching due to the need for small, rapid releases, so to use this strategy effectively, your organization must have minimal viable feature sets. Without the discipline and experience of small releases, the tendency will be to build large, complex features, similar to release branching.
Story or Task Branching

Story or task branching directly connects a user story to changes in source code. It’s the lowest level of branching, and each issue implemented has its own branch, which is typically associated with some user story ID.

Because agile centers around user stories, this type of model is ideal for organizations with a mature agile development process that clearly breaks down stories into small, releasable sets of functionality. This gives a release manager complete flexibility relating to what stories can go with what release, puts no restrictions on how frequently releases can go to production, and limits exposure of defects as far left in the development process as feasible.
Merging Strategies

Branches are intended to be short-lived, making them easy to merge. The ability to frequently (and automatically) merge code is critical to avoiding long, costly merge conflicts. There are three common strategies to merging code from your branches.
Manual Code Review and Merge

The simplest of merging strategies is each branch being manually code reviewed and tested prior to being merged into the master branch. This may work as an initial stage to building out your DevOps capabilities, but it can be plagued with human error and delays. Organizations that adopt this strategy liekly will find that far more defects make their way to the master on average than the other two strategies, but it will still be far less than no branch-merge strategy at all.
Minimal Continuous Integration

Used most commonly in small development projects, this strategy involves using a build orchestration tool, like Jenkins, to compile and test the source code. Often this includes implementing a series of quality gates, or mechanisms that require no human intervention to quantitatively enforce quality, to prevent code that does not pass your tests from being merged into master. Code that passes all quality gates is automatically merged to the master branch.

If your organization has complicated or long-running test suites, this strategy may not be for you. Large feedback cycles can cause bottlenecks and introduce a higher frequency of merge conflicts.
Continuous Integration Pipeline with Quality Gates

This strategy leverages integration branches—or branches that map to stages in your DevOps pipeline—quality gates, and automated merges from your build orchestration tool to ensure bugs and defects are easily identified at particular stages in your pipeline and don’t get merged into the master.

This strategy is helpful for organizations that use a variety of testing types to ensure quality, such as unit tests, functional tests, security tests, regression tests, and load or performance tests. The simplest and easiest to run are incorporated into early stages, while more complex and time-consuming tests are reserved for quality gates (and branches later in your pipeline). This has the benefit of weeding out easy-to-find issues early, making the application of manual testing more efficient and effective.

In practice, we may use branches for development, test, and staging, corresponding to pipeline stages. As code passes the required tests and moves to the next stage in the pipeline, a corresponding merge is made to each branch. This allows us to identify race conditions in integration between multiple features and isolate them before they are found in production. Releases may stop at production, but development can continue until a resolution is made to the defect stage or branch.
Putting It All Together

While there is not one approach that will effectively apply to every organization, the leading best practice is to utilize story branching with automated merges from your quality gates within your continuous integration pipeline.

A DevOps pipeline using Gitflow with recommended testing practices

The graphic above shows a sample DevOps pipeline using Gitflow with recommended testing practices, isolated into three stages: build and integration, quality assurance, and staging and preproduction. Each stage has a series of quality gates to allow a quantitative pass/fail decision. For example, the QA stage requires feature tests, system tests, integration tests, and smoke tests to all pass in order to move to the next stage.

Utilizing the best-practice branch-merge strategy, we would like to have a series of branches that one commit would traverse through to reach production, as depicted in the table below. 

Branch
	

Description
	

Quality gates
	

On success, merges to

Story-132
	

This represents the developers’ initial story branch where changes are made in isolation.
	

Changes are compiled, and unit tests and static code analysis is performed on the branch. If a fast-forward merge is possible without conflict, the code passes all quality gates.
	

Integration

Integration
	

This represents code that has passed development’s “sniff test.” This branch is typically locked down and commits can only be made by the build tool, to ensure developers haven’t gamed the system.
	

Code is again compiled and a deployable package is built for the QA stage. After a deployment, the pipeline would perform smoke tests, feature tests, and system and integration tests. If all were successful, it would pass all quality gates.
	

QA

QA
	

This represents code that has passed the QA stage and is ready for testing in a staging environment.
	

The deployable package is deployed to a staging environment and the pipeline performs security, performance and load, regression, and accessibility testing. If all tests pass successfully, the code passes the quality gates.
	

Staging

Staging
	

This represents code that is ready to be deployed to production.
	

After a manual inspection by a release manager, the code is deployed to production. If the deploy was successful, it passes the quality gate for this stage.
	

Master

There are two major benefits to creating a branch-merge policy for a development organization:

    You have a policy that allows the integration of rapid changes while still enforcing mechanisms to ensure quality, which removes the confusion of moving fast and provides direction to your development team
    Separating changes into small, discrete units can help encourage testing changes in isolation, increasing the odds of identifying bugs and defects earlier in your software development lifecycle

Teams should adopt the best-fitting branch-merge strategy and rely on existing resources and plugins from their build orchestration tools. Over time, the team can iteratively add quality gates and adopt smaller-scoped branches, reducing release sizes and cycle time to bring features to production faster.

The first step is deciding which branch-merge strategy is a fit for your organization, both culturally and technically.



Branching strategies — like feature branching or trunk based development — can help development teams move fast. It can orchestrate parallel development allowing developers to work on tasks simultaneously as part of a team. And parallel builds and testing ensure developers get the feedback they need quickly.

But as projects and teams grow, working in parallel becomes more complex. Because it’s not just about merging changes within a team. Complex products have several teams that need to integrate code.

Within a given team, the “merge early and often” strategy helps combat this issue. But often integrations between teams are pushed later in the development cycle. Bringing all the code together near the end can cause massive merge conflicts and release delays.

But your teams do not have to sacrifice quality for speed. There’s a better code branching strategy.

    By branching and merging code more frequently, your team can be more productive. Implementing the right branching strategy helps support your parallel development efforts (no matter size of your team or project).

This helps you build better products and keeps your codebase stable. Because team members are able to work on portions of the code without impacting others, they can get more done.
Branching Basics

Branching code helps software development teams work in parallel. They can use it to coordinate changes and collaborate on a shared codebase.

To start branching, teams need version control. Version control systems (VCS) organize code, store developers’ work, and allow teams to make and push changes.

When a branch is created, the VCS creates a snapshot of the codebase. And as files are modified, teams can merge back changes. Branches can be made for features, updating frameworks, creating common components, and managing releases.

    Get Started With VCS

    You can get started with a VCS from Perforce — Helix Core — for free. Download Helix Core and start branching better today. 

    Branch With Perforce

What Is a Branching Strategy?

Branching strategies coordinate work to allow for easier integration of changes and releases. They create a development workflow.
Why You Need It

For teams that have hundreds or thousands of developers, branching and merging can be difficult. Bad merges and late-stage integrations suck up developers time, potentially delaying future work or releases.

To reduce the pain (and effort) for your teams, your branching strategy should aim to:

    Optimize productivity.
    Enable parallel development.
    Allow for a set of planned, structured releases.
    Provide a clear promotion path for software changes through production.
    Evolve to accommodate changes that are delivered, perhaps daily.
    Support multiple versions of released software and patches.

What Are Some Branching Strategies?

The point of a branching strategy is to efficiently manage code changes. This workflow will impact both developer and deployment workflows.
Feature Branching Strategy (Task Branching)

    Using a feature branching strategy allows developers to create a branch for a specific feature or task. These are often referred to as user stories. This branch-per-issue workflow allows developers to work separately. 

For example, one team or person might be working on an intrusive bugfix deep in the code, while another might be creating a new workflow for the end user. Each team can work independently on their assigned task and merge changes back into the main branch (mainline) when they're done.
Feature Branching Pros

Allowing developers to experiment and work independently from the core product can keep your codebase stable. This strategy gives your team’s the freedom to innovate, plus you can implement workflows for CI/CD.

It also helps developers easily segment their work. Instead of tackling an entire release, they can focus on small sets of changes. Also feature branches can be divided up according to specific feature groups. Each team or sub-team could maintain their own branch for development. Then when they are finished, changes can be tested and reviewed before integrating them together.

    When using the feature branching strategy for a large enterprise/codebase, it is important to integrate changes across teams frequently.

By integrating all changes earlier in the development cycle, you can prevent costly conflicts that could take a long time to sort out.
Feature Branching Cons (and What to do About it)

The purpose of feature branches is for the branch to live as long as the feature is being developed. This is where a lot of teams struggle. Long-lived feature branches are a nightmare to merge. Because in an effort to avoid conflicts, developers may work in isolation for an extended period of time. But this causes even more issues.

    Featuring branching only works if developers (and teams) branch and merge often.

A lot of developer forums talk about merging at least once a day. But, many teams do not follow this best practice. Plus with most VCS systems, there is no visibility into the main branch. Do you even need to merge? Are new changes available?

To help solve this issue, create an environment for each feature branch for testing. You should also integrate feature branches early in your development cycle. Then you can test and deploy more frequently to ensure there are no issues between branches. Testing production level code helps ensure that code won’t be introduced before it's ready.
Feature Flag Branching Strategy for Continuous Delivery

The goal of DevOps teams is to test by building often. Continuously integrating, testing, and delivering code back to developers ensures that teams fail fast, and resolve issues quickly.

    To help support this type of development, some teams implement feature toggles or flags, instead of maintaining a separate feature branch.The advantage is that all work can be done right from in the mainline. This means less branches and minimal merging.

By using the feature toggle, portions of the code can be turned on or off for the build process. Feature flags can help teamsscale quickly. But it slows them down over time as projects and teams grow.

If you have a monolith codebase, this can be costly to maintain. Flags need to be added for each new feature on the code level. And admins need to write scripts to manage them. This takes time away from DevOps teams enhancing build automation that can really help you deliver faster.

[Related Blog: How to Optimize Your Software Delivery Pipeline]
Release Branching Strategy

    A release branching strategy involves creating a branch for a potential release that includes all applicable stories. When a team starts working on a new release, the branch is created.

For teams that need to support multiple releases and patch versions over time, a release branching strategy is required. Teams can work on all user stories within the mainline branch for that specific release.

It is common to follow different branching strategies depending on the type of release. For example, major releases may use a feature/team-based branching strategy on top of the release branch. Whereas patches/hotfixes may work directly in the release branch.
Release Branching Pros

If you are working on a product that needs to support multiple versions in parallel or needs to handle customization for a specific customer, release branching is a requirement. It allows your team to focus on specific issues per patch or release.
Release Branching Cons (and What to do About it)

Working with a lot of release branches can be difficult to maintain. If you have a lot of changes and contributors, your codebase can quickly become unstable.

It also can potentially create more work for teams. If they need to maintain multiple releases, changes would need to be applied into several versions. For example, you could be supporting version 1.0, 2.0, 3.0., etc.

If you are managing multiple parallel versions or customizations of your software, it is critical to have a process in place. It should ensure that bugfixes are propagated, merged, and tested across the relevant release branches to avoid regressions. 
Merging Strategies 101

Whether you use task/feature branching, release branching, or a combination of branching strategies, at the end of the branch, you merge.

Depending on your VCS, your workflow will vary. With both centralized and distributed systems, you usually end up merging everything to one server. Having a single source of truth helps your DevOps teams build easier. They can get everything they need for a build from one central location.

And to help you avoid ‘merge-hell,’ we have some tips to help you branch better.
Branching Best Practices For All Branching Strategies
Know and Communicate Your Branching Strategy for a Project

Once you decide on a branching strategy, you need to document it and communicate it to your team. You want to outline:

    When a developer should branch? From where?
    When they should merge (and how often)? To where?

Defining this workflow across teams is vital. This is usually done through documentation, folder structures, or on a whiteboard. Throughout the development cycle, check in with teams to make sure everyone is on the same page.
Minimize How Long Code Is Checked Out

We are saying it again for the developers in the back! Some teams have a lot of branches, and some have a few. But no matter how many you have, limiting the time code is checked out helps prevent merge conflicts.

Developers should regularly copy down code to ensure they have the most up-to-date files. The longer something is checked out, the more isolated that code can become.
Figure Out Your Dependencies

If you are working on a hotfix, or even a feature, you need to know what versions or teams it will impact. Before you branch, it is important to think about how changes need to be propagated, and where code needs to be integrated.
Review Your Merge/Integration Process

Let’s say a developer is making multiple changes on a branch. When they go to merge, the build breaks. They fix it. And then just before the release, code is integrated across teams. Now BANG, everything is broken.

But which change caused the issue?

The more code you have to sort through, the longer a merge or integration will take. Merging often reduces the risk that something breaks. Integrating code earlier, or integrating across teams one at a time, allows contributors to quickly isolate an issue.
Pick the Right Version Control System

When you think about picking a VCS system, branching is only one part. You want a system that can handle the number of files, contributors, and build demands. Also you are going to need performance to back it up.

And although branching my never be completely stress-free. It doesn’t have to be as much of a headache if you choose the right VCS. With Helix Core –– version control from Perforce –– you get Perforce Streams. This branching method shows the relationships between branches and has branching best practices built-in. It offers large teams a better way to branch and merge.
Gain Speed With Perforce Streams

Streams makes developers life easier. It helps them code more instead of dealing with overhead. And admins don’t need to spend time implementing workflows, complicated scripts, and sending angry emails.

With Streams, the structure is built-in to support complex branching for development and releases (no matter your strategy). Plus this foundation is flexible enough to adapt to how your team works.
How Perforce Streams Works

When a developer creates a branch, Streams sets up their workspace automatically. It knows what files are available and developers can see which ones are being worked on (and by whom).

    Streams creates and shows the flow of change between branches.

Streams Branching Strategy for Multiple Releases

Your team no longer needs to rely on naming conventions to determine how branches are related. And forget about documenting branching strategies on a white board! Because with Streams, you always know where your code needs to go.

Using the Stream Graph, developers can visualize how code is propagated. They can quickly check to see if they have the most updated version of the mainline or parent branch. If there are updates available, new code can be merged down. Then changes can be copied up. This helps keep your branches stable.

It’s that simple.

Remove complications with branching with Streams. It does the work for you. See for yourself.




Branching and Merging: Ten Pretty-Good Practices

In the course of rescuing a development from 'merge misery', it became increasingly apparent that there were a number of practices for managing branches in the Version Control System that would have reduced the pain and effort of the subsequent merge, and made the dream of continuous delivery come closer to reality. From the experience comes some well-tested ways of making branches and merges a stress-free part of application development.

DevOps, Continuous Delivery & Database Lifecycle Management
Automated Deployment
A bit of history…

“Konrad! We have to talk!” – With those words, the Programme Manager rushed into my office room. I don’t like it when Programme Managers move fast: it is usually a bad sign. “We have an issue with the release of the next module of our financial system. Its’ code refers to some core libraries, which we have significantly modified since last release. Now those amendments need to be merged with the live version of module. You will lead a team that will work on the integration task”.

I already knew about this issue and many others as well, because I’d been in the programme for more than nine months. I was keen to do the job as  I had heard complaints on many occasions from other architects about the many technical aspects that had not been cared for properly, or endlessly postponed because they’d been valued less than business features. It wasn’t divine intervention that led the customer to suddenly realize that those things mattered: It was simply that the deadline for the release was looming closer and the ground was starting to slide from under the customer’s feet. I set about building a list of the initial backlog of work in collaboration with my colleagues. It was only the tip of the iceberg, and at that stage we had no idea of the extent of the ice lurking underneath. We had to:

    combine the changes in the database schema and the data integration logic between modules, each of which had evolved this code in different and mutually-exclusive way
    evolve the folder-structure of branches for relatively huge codebase to common format, so that the folders are named consistently and located in the same place of directory tree
    upgrade 3rd party libraries, the CMS framework and our core libraries in all modules to the same version
    write additional integration tests and, where that proves too costly, perform manual regression in order to confirm that the result of our merge is stable
    resolve deployment issues, determine where release steps had not been automated and automate them

Furthermore during the process we discovered that our deployment scripts were unmaintainable. When we tried to fix them, we entered a vicious cycle – one uncovered problem lead to another. At some point builds started to take more than forty minutes aggravating further our situation.

After a few months of unforeseen work at a frantic pace, we successfully delivered the project, largely thanks to a great team. From the history of that struggle emerged the most important lesson – a vast amount of effort and problems can be avoided by having a proper branching strategy and by a continuous integration (CI) process. Here are ten pretty-good practices, which I found useful.
Pretty-good Practices for Branching and Merging
Use the standard Source Control folder-structure correctly

In the development environment I work in, SVN is very popular version control system (VCS). I noticed that several problems were caused by developers not understanding the standard folder-structure and its’ purpose. We have three main folders.

    Trunk, the first one, is usually used in two ways: as a read-only reflection of ‘live’ or as an active development branch.
    Tags is meant to be read-only, immutable set of shortcuts to shipped releases and it should never be used for coding.
    Finally, Branches folder should contain: hotfixes, experimental branches and the intermediate results of parallel feature development.

Know the strategy used in your project

Communication will be clearer, and misunderstandings will be less frequent, if the team uses the terminology and jargon correctly and consistently. It is not just the architect who is responsible for promoting this, but the entire delivery team. For example, we have three strategies, which I believe every member of the development team should be aware of.

    “Branch by release“, sometimes called “staircase model” is the oldest one, where for every planned release we have a separate branch.
    “Branch by feature“, in turn, has distinct branches for different user stories (US). This occasionally comes in handy, as when we have to do a challenging task such as a cross-cutting upgrade, and we do not want to risk the stability of our code by mixing upgrade check-ins with other stuff.
    Lastly, we have the “branch by abstraction” approach, which has recently had much more attention due to the “continuous delivery” (CD) buzz. This strategy is founded on the concept of a single branch for everything. Features that are new, but not yet finished, can be enabled or disabled by feature-toggles. We can use this device to release different versions of our application from single branch, by changing its configuration. It is very convenient, because it frees us from the merging task, but it is not appropriate for every project.

Try to minimize the number of branches

No matter which strategy is in use, it is always beneficial to minimize the number of branches that are required. Fewer branches means less merges, reduced conflict and diminished misunderstanding within a team, in the same way that, the fewer the layers of the architecture, the lower the cost. In the extreme case, this might be reduced to a single branch with feature toggles. However, on larger code-bases, branching allows changes to be isolated, and so helps teams to avoid disturbing each other. Also, short-lived feature branches can work well, especially when used for risky refactoring or experimentation. It is only when the branches remain in existence for too long that the likelihood of merge-conflicts mount.
Predict release dependencies

When working with a few branches of code that reflect subsequent releases, it is good to be able to predict dependencies and make decisions that will make subsequent merges as easy as possible. There are questions that need to be answered as soon as possible. Which release will go first? What will be the direction of merges and their frequency? On which branch should a particular core component be implemented so that it is most useful and less risky to everybody? If left unanswered, these become so-called “last mile” delivery problems. We have to try to foresee potential issues and plan branches / merges accordingly.
Do merges regularly

Deployments are still sporadic in the enterprise environment, even though start-ups and community projects are revolutionizing delivery techniques. It will take time to change the release strategy for the majority of companies. As a consequence, merges might happen rarely and I’ve already alluded to the sort of problems that this leads to. Even if the release cycle is extended, it is much better to do the merges regularly; not necessarily automatically, because you then lose the chance to review the code and get familiar with changes of other teams. I recommend weekly merges as being a good frequency to start with. Those merges should be performed by the team that is responsible for the target branch of the merge – they know best when to do it and how to resolve any conflicts.
Think about the impact of the choice of repository

Every VCS has its advantages and drawbacks. This will affect the decisions you take about the merging process, and the Source-Control strategy you choose. SVN or Git manage merging and branching better than TFS. TFS’s Auto-merge feature is poorer than in competing products and occasionally the results are so bad that some my colleagues decided to not use this feature at all. Moreover, there is no possibility of synchronizing three branches – the tool will always detect some difference, even though it is only in the metadata. TFS tried to prevent misuse of merging by limiting this process to branches that are in parent-child relationship. Although this initially seems like a safer option, in reality it is restrictive. TFSs’ “Baseless merge” is a way to work around this, but it is an alternative that is available only from the command-line and so leaves users with the feeling that you need many “hacks” to work with the tool. My experience comes from work with the 2010 edition, so please bear in mind that this might already be fixed.
Coordinate changes of shared components

Merges might be difficult, taking days and requiring expertise in code. It is occasionally almost impossible, especially for binary files or XML, such as the one used by SSIS’s DTSX files. That is why we have to avoid them where possible by planning and coordinating the changes to elements of the application that pose a risk, especially shared elements such as the database or core libraries. Mitigation techniques can include:

    Brain-storming sessions to identify all possible implementation options
    Organizing meetings to discuss the impact of changes
    Creating core teams that are responsible for shared components

At Objectivity, one of teams that faced a particularly complex challenge used a branch graph, hanging on the wall, to discuss and plan future releases. I really liked that practice.
Develop methods to simplify merges

Undoubtedly, there will be times when merging can’t be avoided. Certain methods may help to shorten the time before a merge, and minimize the number and scope of code-conflicts. It may help to structure the folders to isolate the code on which teams will work in parallel.

When continuously writing database upgrade scripts, each identified with unique version, you may come across script version conflict during merge. It will happen for example when teams, working independently on separate branches, have written two scripts upgrading to the same DB version, but containing different database changes. With more file conflicts situation is even worse as usually upgrades are not idempotent and they have to be run in planned sequence, giving you no place in between to insert conflicting scripts. To prevent, simply reserve individual version ranges for the teams. Alternatively use separate schemas or databases so that each team owns part of DB. That helps to mitigate conflicts, but is typically not sufficient in enterprise apps requiring data sharing.

When system gets large, it is likely to need to be modularized; and its components, whether shared or not, should be kept in separate branches, compiled into binary form (DLL or NuGet for example) and used in dependent projects as external libraries. This way component’s code is held in single place, it does not require merging and the build is faster as the components are compiled just once. If those methods fail or are too costly, the fall-back is described above change coordination.
Promote “CI for everybody”

Every active branch should have a separate CI process to ensure its quality and to make it easy to change and maintain. Without CI and automated tests, the system becomes harder to modify duue to the prolonged feedback loop. If branches survive too long without a merge, then the merge-related bugs are discovered too late, when some of the detail about the changes within the branch has been forgotten. This rule is very intuitive, yet not always followed.
Include merging in estimation

Good agile teams use “definition of done” (DoD) to decide when particular feature can be considered as “done”, so that there are fewer misunderstandings within a team or between a team and customer about progress. I found it useful to consider whether merging activity should be included into DoD: Sometimes it is irrelevant or obvious, but at times it counts a lot.